{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main libraris\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import os, sys, shelve, string, time, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, OrdinalEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "from category_encoders.backward_difference import BackwardDifferenceEncoder\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from category_encoders.helmert import HelmertEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from category_encoders.james_stein import JamesSteinEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from category_encoders.hashing import HashingEncoder\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "from category_encoders.polynomial import PolynomialEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support function\n",
    "from typing import List\n",
    "class DoubleValidationEncoderNumerical:\n",
    "    \"\"\"\n",
    "    Encoder with validation within\n",
    "    \"\"\"\n",
    "    def __init__(self, cols: List, encoder, folds):\n",
    "        \"\"\"\n",
    "        :param cols: Categorical columns\n",
    "        :param encoder: Encoder class\n",
    "        :param folds: Folds to split the data\n",
    "        \"\"\"\n",
    "        self.cols = cols\n",
    "        self.encoder = encoder\n",
    "        self.encoders_dict = {}\n",
    "        self.folds = folds\n",
    "\n",
    "    def fit_transform(self, X: pd.DataFrame, y: np.array) -> pd.DataFrame:\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        for n_fold, (train_idx, val_idx) in enumerate(self.folds.split(X, y)):\n",
    "            X_train, X_val = X.loc[train_idx].reset_index(drop=True), X.loc[val_idx].reset_index(drop=True)\n",
    "            y_train, y_val = y[train_idx], y[val_idx]\n",
    "            _ = self.encoder.fit_transform(X_train, y_train)\n",
    "\n",
    "            # transform validation part and get all necessary cols\n",
    "            val_t = self.encoder.transform(X_val)\n",
    "\n",
    "            if n_fold == 0:\n",
    "                cols_representation = np.zeros((X.shape[0], val_t.shape[1]))\n",
    "            \n",
    "            self.encoders_dict[n_fold] = self.encoder\n",
    "\n",
    "            cols_representation[val_idx, :] += val_t.values\n",
    "\n",
    "        cols_representation = pd.DataFrame(cols_representation, columns=X.columns)\n",
    "\n",
    "        return cols_representation\n",
    "\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        X = X.reset_index(drop=True)\n",
    "\n",
    "        cols_representation = None\n",
    "\n",
    "        for encoder in self.encoders_dict.values():\n",
    "            test_tr = encoder.transform(X)\n",
    "\n",
    "            if cols_representation is None:\n",
    "                cols_representation = np.zeros(test_tr.shape)\n",
    "\n",
    "            cols_representation = cols_representation + test_tr / self.folds.n_splits\n",
    "\n",
    "        cols_representation = pd.DataFrame(cols_representation, columns=X.columns)\n",
    "        \n",
    "        return cols_representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-loader\n",
    "from zipfile import ZipFile\n",
    "def loader(path, index_col=False):\n",
    "    \"\"\"\n",
    "    Unpack kaggle zip-data, then return dict of pd.data\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: current path to folder with data\n",
    "        String\n",
    "    \n",
    "    index_col: Column to use as the row labels of the DataFrame, either given as string name or column index.  \n",
    "    If a sequence of int / str is given, a MultiIndex is used.\n",
    "    Note: index_col=False can be used to force pandas to not use the first column as the index, e.g. when \n",
    "    you have a malformed file with delimiters at the end of each line. \n",
    "        int, str, sequence of int / str, or False, default None\n",
    "        \n",
    "    Future:\n",
    "    encoding='utf-8' parameter for open method\n",
    "\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    for i in os.listdir(path):\n",
    "        if os.path.splitext(os.path.join(path, i))[1] == \".zip\":\n",
    "            with ZipFile(os.path.join(path, i), 'r') as g:\n",
    "                file_list = g.namelist()\n",
    "                for file_name in file_list:\n",
    "                    if file_name.endswith('.csv'):\n",
    "                        with g.open(file_name) as h:\n",
    "                            filename = os.path.splitext(file_name)[0]\n",
    "                            data_dict[filename] = pd.read_csv(h, index_col=index_col)\n",
    "        elif os.path.splitext(os.path.join(path, i))[1] == \".csv\":\n",
    "            with open(os.path.join(path, i), 'r') as g:\n",
    "                filename = os.path.splitext(i)[0]\n",
    "                data_dict[filename] = pd.read_csv(g, index_col=index_col)            \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    \"\"\"\n",
    "    Reduse numeric \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas data frame\n",
    "        pd.DataFrame object\n",
    "\n",
    "    \"\"\"\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searching for unical ordered value\n",
    "def searchFunc(data, *cols):\n",
    "    \"\"\"\n",
    "    Function return dictionary of the form: 'value': index, that can be used for \n",
    "    mapping in ordered feature encoding estimators\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data: pandas data frame\n",
    "        pd.DataFrame object\n",
    "    \n",
    "    cols: list of columns, where function search for unical ordered value \n",
    "        list, tuple\n",
    "    \n",
    "    \"\"\"\n",
    "    full_map = []\n",
    "    for i in cols:\n",
    "        mapping = {}\n",
    "        for idx, val in enumerate(pd.unique(sorted(data[i]))):\n",
    "            mapping[val] = idx\n",
    "        full_map.append(mapping)\n",
    "    return full_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dump from this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader(os.path.realpath('../input'))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x, df_train_y, df_test_x, df_test_y = data.values()\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump loaded and prepared data\n",
    "with shelve.open(os.path.realpath('../kernels/loaded_data')) as s:\n",
    "    s[\"df_train\"] = df_train_x\n",
    "    s[\"df_test\"] = df_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = reduce_mem_usage(df_train_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump open (prepared dataset)\n",
    "with shelve.open(os.path.realpath('../kernels/loaded_data')) as o:\n",
    "    df_train = o[\"df_train\"]\n",
    "    df_test = o[\"df_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing pipline\n",
    "pipePre = Pipeline([\n",
    "    ('simpleimputer', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n",
    "    ('standardscaler', StandardScaler()),\n",
    "    ('normalizer', Normalizer())\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x = pipePre.fit_transform(df_train_x)\n",
    "df_test_x = pipePre.fit_transform(df_test_x)\n",
    "del df_train_x\n",
    "del df_test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_train_x, df_train_y, test_size = 0.25, random_state=42)\n",
    "N_train, _ = X_train.shape \n",
    "N_test,  _ = X_test.shape \n",
    "print(N_train, N_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
